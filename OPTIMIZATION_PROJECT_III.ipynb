{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e35f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11aa3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeLimit = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4c6f0",
   "metadata": {},
   "source": [
    "## Problem Overview\n",
    "One of the most common problems in predictive analytics is variable selection for regression. Direct variable selection using optimization has long been dismissed by the statistics/analytics community because of computational difficulties. This computational issue was part of the motivation for the development of LASSO and ridge regression. However, in the recent past there have been tremendous advancements in optimization software, specifically the ability to solve mixed integer quadratic programs (MIQP). This project will pose the variable selection problem for regression as an MIQP which you will solve using gurobi. You will compare the results you find to LASSO to see if the additional ‘shrinkage’ component of LASSO really is more beneficial than finding the ‘best’ set of variables to include in your regression.\n",
    "\n",
    "## Direct Variable Selection – MIQP Problem\n",
    "Given a dataset of $m$ independent variables, $X$, and a dependent variable, $y$, the standard ordinary least squares problem is formulated as\n",
    "\n",
    "$$\\min_\\beta \\sum^{n}_{i=1}(\\beta_0+\\beta_1 x_{i1}+...+\\beta_m x_{im}-y_i)^2$$\n",
    "\n",
    "In order to incorporate variable selection into this problem we can include some binary variables, $z_j$, that force the corresponding values of $\\beta$ to be zero if $z_j$ is zero, using the big-M\n",
    "method that we discussed in class, and used in the previous project. If we only want to include at most $k$ variables from $X$, then we can pose this as\n",
    "\n",
    "$$\\min_{\\beta,z} \\sum^{n}_{i=1}(\\beta_0+\\beta_1 x_{i1}+...+\\beta_m x_{im}-y_i)^2$$\n",
    "$$s.t. -Mz_j\\leq\\beta_j\\leq Mz_j, for j=1,2,...,m$$\n",
    "\n",
    "$$\\sum_{j=1}^m z_j\\leq k$$\n",
    "$$z_j=binary$$\n",
    "\n",
    "Note that we don’t ever forbid the model from having an intercept term, $\\beta_0$, and that $m$ and $M$ are different things here. Here, $k$ can be viewed as a hyperparameter to be chosen using cross validation.\n",
    "\n",
    "In order to pose this in the standard framework of a quadratic programming objective let’s see how we can rewrite this objective using linear algebra. Let $\\beta$ be an $(m+1) \\times 1$ column vector that contains $\\beta_0,...\\beta_m$, let X be the $n \\times (m+1)$ matrix that has its **first column made up entirely of 1s**, and columns 2 to (m+1) have the data from the m independent variables, and let y be the $n\\times 1$ column vector that has the dependent variable data. You can use np.array to convert the pandas dataframe to a matrix and then add a column of all 1s to the matrix. Then we can create an n x 1 vector whose entries are the n values inside the parentheses from the problem statement by doing the following matrix calculation: $(X\\beta − y)$. Then if we want to take the sum of squared entries of this vector, we can multiply $(X\\beta − y)^T ∗ (X\\beta − y)$. Using a few tricks from linear algebra we can pose the optimization problem’s objective function as\n",
    "\n",
    "$$\\min_{\\beta,z}\\beta^T(X^TX)\\beta + (-2y^TX)\\beta$$\n",
    "\n",
    "The only issue left to be resolved is that the vector of decision variables needs to be of size $(2m+1)\\times 1$; made up of the $(m+1)$ values of $\\beta$ and the $m$ values of $z$, but the objective written above only includes the $m+1$ values of $\\beta$. To fix this we can assign the $Q$ matrix to be a $(2m+1) \\times (2m+1)$ matrix where the upper left corner of the matrix is equal to $X^TX$, and all other values are zero. We also need the linear term of the objective to be a $(2m+1)\\times 1$ vector where the first $(m+1)$ components are $-2y^TX$, and the rest are zeros. Now if you create the constraint matrix and right-hand-side vector from the constraints given above, you have all you need to solve the problem using gurobi.\n",
    "\n",
    "## Indirect Variable Selection – LASSO\n",
    "The LASSO version of regression is posed as\n",
    "$$\\min_\\beta \\sum^{n}_{i=1}(\\beta_0+\\beta_1 x_{i1}+...+\\beta_m x_{im}-y_i)^2+\\lambda \\sum^m_{j=1}|\\beta_j|$$\n",
    "where $\\lambda$ is a hyperparameter to be chosen using cross-validation. It turns out that if $\\lambda$ is large enough, several values of $\\beta$ will be forced to be equal to zero. This model also has the benefit of ‘shrinking’ the $\\beta$s closer to zero, which achieves variance reduction (prevents overfitting). Note again that $\\beta_0$ is not included in the $\\lambda$ sum. You should never penalize a model for having an intercept term. The standard package in Python to solve the LASSO problem is scikit learn. In this project you will need to use scikit learn to solve the LASSO problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41156c",
   "metadata": {},
   "source": [
    "## 1\n",
    "One data set is a training data set, and one is a test data set. You will follow the data science pipeline carefully here. You will first do 10-fold cross validation on the training set to pick $k$ or $\\lambda$. Then using the optimal values of $k$ or $\\lambda$ you will fit your $\\beta$s using the entire training set. Then with those $\\beta$s you will make a prediction of the y values on the test set, and compare your prediction of y, to the true value of y in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77577e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('training_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f5e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 51) (50, 51)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fb91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(train['y'])\n",
    "X = np.array(train.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a60137",
   "metadata": {},
   "source": [
    "## 2\n",
    "In order to do cross validation on the MIQP model you will have to write your own cross validation code. Randomly shuffle your data and split it into 10 folds (‘np.random.choice()’). There are 50 X variables, and you will need to try k = 5, 10, 15, ..., 50 in your cross validation. This means to do 10-fold cross validation with all possible values of k, you will have to solve an MIQP model 100 times! Pick the value of k that corresponds to the smallest cross validation error: for a given value of k, sum each validation set’s sum of squared errors using the $\\beta$s found using the other 9 folds’ data to solve the MIQP. When k is 5 or 50, gurobi should solve the problem pretty quickly, but when k is 25 it will probably take a long time. Therefore, you should set a time limit for gurobi to solve each problem. Don’t let the entire process run for any longer than 12 hours. \n",
    "\n",
    "> a. Set lb value of your model to be -M for the appropriate decision variables.\n",
    "\n",
    "> b. Choose M to be large enough so that no value of $\\beta$ is equal to M or -M. If you solve the problem and one of your $\\beta$s is M or -M then you should double M and resolve the problem. Repeat until no $\\beta$ is equal to M or -M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54e549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc5d756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    k         sse\n",
      "0   5  917.479061\n",
      "1  10  724.787631\n",
      "2  15  764.049938\n",
      "3  20  799.003940\n",
      "4  25  770.482828\n",
      "5  30  830.082402\n",
      "6  35  832.484731\n",
      "7  40  847.343751\n",
      "8  45  843.258526\n",
      "9  50  847.184545\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('k_sse.csv'):\n",
    "    k_sse = pd.read_csv('k_sse.csv') \n",
    "    print(k_sse)\n",
    "else:\n",
    "    ks = [x for x in range(5,51,5)]\n",
    "    kf = KFold(n_splits=10)  # if we need other number of folds, change it here\n",
    "    error_dict = {}\n",
    "\n",
    "    for k in ks:\n",
    "        error = 0\n",
    "        for train_index, test_index in kf.split(X):\n",
    "\n",
    "            X_train, X_validate = X[train_index], X[test_index]\n",
    "            y_train, y_validate = y[train_index], y[test_index]\n",
    "\n",
    "            # big M\n",
    "            M = 100\n",
    "\n",
    "            # m and n\n",
    "            n, m = X_train.shape\n",
    "\n",
    "            # add column of 1's in X\n",
    "            X_Q = np.ones((n,m+1))\n",
    "            X_Q[:,1:] = X_train\n",
    "\n",
    "            # quadratic objective\n",
    "            Q = np.zeros((2*m+1, 2*m+1))\n",
    "            Q[:m+1, :m+1] = X_Q.T @ X_Q\n",
    "\n",
    "            # linear objective\n",
    "            C = np.zeros(2*m+1)\n",
    "            C[:m+1] = (-2)*y_train.T @ X_Q\n",
    "\n",
    "            # decision variables\n",
    "            # beta0, beta1, ..., betam, z1, ..., zm\n",
    "            vtypes = np.array(['C']*(m+1) + ['B']*m)\n",
    "            lb = np.array([-np.inf] + [-M]*m + [0]*m)\n",
    "            \n",
    "            # constraint matrix\n",
    "            A = np.zeros((2*m+1, 2*m+1))\n",
    "            A[0, m+1:] = [1]*m                    # z1 + ... + zm ≤ k\n",
    "            np.fill_diagonal(A[1:m+1, 1:m+1], 1)  # coefficients for beta_j\n",
    "            np.fill_diagonal(A[1:m+1, m+1:], M)   # beta_j + Mz_j ≥ 0\n",
    "            np.fill_diagonal(A[m+1:, 1:m+1], 1)   # coefficients for beta_j\n",
    "            np.fill_diagonal(A[m+1:, m+1:], -M)   # beta_j - Mz_j ≤ 0\n",
    "\n",
    "            b = np.zeros((2*m+1,1))\n",
    "            b[0] = k\n",
    "\n",
    "            sense = np.array(['<'] + ['>']*m + ['<']*m)\n",
    "\n",
    "            miqpMod = gp.Model()\n",
    "            miqpMod_x = miqpMod.addMVar(2*m+1, vtype=vtypes, lb=lb)\n",
    "            miqpMod_con = miqpMod.addMConstr(A, miqpMod_x, sense, b)\n",
    "            miqpMod.setMObjective(Q,C,0,sense=gp.GRB.MINIMIZE)\n",
    "            miqpMod.Params.OutputFlag = 0 \n",
    "            miqpMod.Params.TimeLimit = timeLimit\n",
    "            miqpMod.optimize()\n",
    "\n",
    "            betas = np.array(miqpMod.x[:m+1])\n",
    "\n",
    "            n_test, m_test = X_validate.shape\n",
    "\n",
    "            X_Q2 = np.ones((n_test,m_test+1))\n",
    "            X_Q2[:,1:] = X_validate\n",
    "\n",
    "            error += (X_Q2 @ betas - y_validate).T @ (X_Q2 @ betas - y_validate)\n",
    "\n",
    "        error_dict[k] = error\n",
    "        print(k, ':', error)\n",
    "    best = min(error_dict, key=error_dict.get)\n",
    "    print('Best k: {}, SSE: {:.4f}'.format(best, error_dict[best]))\n",
    "    \n",
    "    keys = [x[0] for x in error_dict.items()]\n",
    "    values = [x[1] for x in error_dict.items()]\n",
    "    k_sse = pd.DataFrame.from_dict({'k':keys, 'sse':values})\n",
    "    k_sse.to_csv('k_sse.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb7aff",
   "metadata": {},
   "source": [
    "## 3\n",
    "Once you find the k with the smallest cross validation error, fit the MIQP model on the entire training set using that value of k. Use the $\\beta$s you find in this MIQP to make a prediction of the y values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd56f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k = int(k_sse['k'][k_sse['sse'] == k_sse['sse'].min()])\n",
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7160724",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.iloc[:, 1:], train['y']\n",
    "\n",
    "n, m = X_train.shape\n",
    "M = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b6bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column of 1's in X\n",
    "X_Q = np.ones((n,m+1))\n",
    "X_Q[:,1:] = X_train\n",
    "\n",
    "# quadratic objective\n",
    "Q = np.zeros((2*m+1, 2*m+1))\n",
    "Q[:m+1, :m+1] = X_Q.T @ X_Q\n",
    "\n",
    "# linear objective\n",
    "C = np.zeros(2*m+1)\n",
    "C[:m+1] = (-2)*y_train.T @ X_Q\n",
    "\n",
    "# decision variables\n",
    "# beta0, beta1, ..., betam, z1, ..., zm\n",
    "vtypes = np.array(['C']*(m+1) + ['B']*m)\n",
    "lb = np.array([-np.inf] + [-M]*m + [0]*m)\n",
    "\n",
    "# constraint matrix\n",
    "A = np.zeros((2*m+1, 2*m+1))\n",
    "A[0, m+1:] = [1]*m \n",
    "np.fill_diagonal(A[1:m+1, 1:m+1], 1)\n",
    "np.fill_diagonal(A[1:m+1, m+1:], M)\n",
    "np.fill_diagonal(A[m+1:, 1:m+1], 1) \n",
    "np.fill_diagonal(A[m+1:, m+1:], -M) \n",
    "\n",
    "b = np.zeros((2*m+1,1))\n",
    "b[0] = best_k\n",
    "sense = np.array(['<'] + ['>']*m + ['<']*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e443466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-08-17\n"
     ]
    }
   ],
   "source": [
    "miqpMod = gp.Model()\n",
    "miqpMod_x = miqpMod.addMVar(2*m+1, vtype=vtypes, lb=lb)\n",
    "miqpMod_con = miqpMod.addMConstr(A, miqpMod_x, sense, b)\n",
    "miqpMod.setMObjective(Q,C,0,sense=gp.GRB.MINIMIZE)\n",
    "miqpMod.Params.OutputFlag = 0 \n",
    "miqpMod.Params.TimeLimit = timeLimit\n",
    "miqpMod.optimize()\n",
    "\n",
    "beta = np.array(miqpMod.x[:m+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16642ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1057453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 10, SSE = 116.82720, MSE = 2.33654\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = test.iloc[:, 1:], test['y']\n",
    "n_test, m_test = X_test.shape\n",
    "\n",
    "X_Q = np.ones((n_test,m_test+1))\n",
    "X_Q[:,1:] = X_test\n",
    "\n",
    "sse = (X_Q @ beta - y_test).T @ (X_Q @ beta - y_test)\n",
    "mse = mean_squared_error(y_test, X_Q @ beta)\n",
    "\n",
    "print('k = {}, SSE = {:.5f}, MSE = {:.5f}'.format(best_k, sse, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecdded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fd863d6",
   "metadata": {},
   "source": [
    "## 4\n",
    "Use scikit learn to do 10-fold cross validation on the training set to pick $\\lambda$. Once you find the best value of $\\lambda$, fit a LASSO model to the entire training set using that value of $\\lambda$. With the $\\beta$s you find in that LASSO model make a prediction of the y values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e95f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adf6e3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.0057453437864455085\n",
      "Number of variables: 18\n",
      "[-0.         -0.          0.          0.         -0.          0.\n",
      " -0.         -0.         -2.11561506  0.         -0.06043079 -0.\n",
      " -0.         -0.         -0.41674549 -0.18155256  0.          0.\n",
      " -0.          0.          0.         -0.19710223 -1.3655275   0.73510021\n",
      " -0.         -1.30018578  0.          0.          0.06390289  0.\n",
      " -0.          0.         -0.10737966  0.25392747  0.02138366  0.\n",
      "  0.          0.         -0.21159473  0.         -0.          0.\n",
      "  0.          0.01152326  1.53171531 -0.01408773  0.6504778  -0.09757869\n",
      "  0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louie/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lasso = LassoCV(cv=10, normalize=True).fit(X_train, y_train)\n",
    "\n",
    "alpha, beta = lasso.alpha_, lasso.coef_\n",
    "print('Best lambda: {}\\nNumber of variables: {}'.format(alpha, (beta != 0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed8076a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0.00575, MSE = 2.35971\n"
     ]
    }
   ],
   "source": [
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "mse_lasso = mean_squared_error(y_test, y_pred)\n",
    "print('Lambda = {:.5f}, MSE = {:.5f}'.format(alpha, mse_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408d39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
