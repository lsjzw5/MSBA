{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peyton Lewis, Louie Wang, Anushka Iyer, Leyang Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was a second attempt after our DQN modeling attempts. We were able to obtain a model that won against a random opponent more often with the Policy Gradients models, and therefore show this PG code as well. At the bottom of the script is a section where our submitted h5 file can be tested against a random opponent. This is reliant on our valid_moves() function, which is also present in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods Implemented: \n",
    "- Policy Gradients (softmax final layer)\n",
    "- Self-play network training (flip coin which player goes first each time)\n",
    "- Linear Annealing strategy for epsilon\n",
    "- Memory buffer \n",
    "- Auto-Win and Auto-Blocking moves (fractional percentage of the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that updates the game board once action is chosen\n",
    "\n",
    "def update_board(board_temp,color,column):\n",
    "    # this is a function that takes the current board status, a color, and a column and outputs the new board status\n",
    "    # columns 0 - 6 are for putting a checker on the board: if column is full just return the current board...this should be forbidden by the player\n",
    "    # columns 7 - 13 are for pulling a checker off the board: this does not check if removing the checker is allowed...\n",
    "    # \n",
    "    # the color input should be either 'plus' or 'minus'\n",
    "    \n",
    "    board = board_temp.copy()\n",
    "    ncol = board.shape[1]\n",
    "    nrow = board.shape[0]\n",
    "    if column < ncol: # drop a checker on the board\n",
    "        row = -1\n",
    "        # start by assuming you can't add to the column\n",
    "        # loop through the rows checking if you can go to each row or not\n",
    "        for check in range(nrow):\n",
    "            if (board[check,column]!=0):\n",
    "                break # if this row is occupied, you're done\n",
    "            else: # otherwise, you can go on this row!\n",
    "                row += 1\n",
    "\n",
    "        if row >= 0: # if you can add to the column\n",
    "            if color == 'plus': # check the color\n",
    "                board[row,column] = 1\n",
    "            else:\n",
    "                board[row,column] = -1\n",
    "        return board\n",
    "    else:\n",
    "        column -= ncol\n",
    "        if column >= ncol:\n",
    "            return board # can't play anything bigger than 13...\n",
    "        board[1:,column] = board[:-1,column].copy()\n",
    "        if board[0, column] != 0: # if column completely full, top spot is a 0\n",
    "            board[0, column] = 0\n",
    "        return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that checks for a winner on the board\n",
    "\n",
    "def check_for_win(board):\n",
    "    # this function checks to see if anyone has won on the given board\n",
    "    nrow = board.shape[0]\n",
    "    ncol = board.shape[1]\n",
    "    winner = 'nobody'\n",
    "    for row in range(nrow):\n",
    "        for col in range(ncol):\n",
    "            # check for vertical winners\n",
    "            if row <= (nrow-4): # can't have a column go from rows 4-7...\n",
    "                if np.sum(board[row:(row+4),col])==4:\n",
    "                    winner = 'v-plus'\n",
    "                    return winner\n",
    "                elif np.sum(board[row:(row+4),col])==-4:\n",
    "                    winner = 'v-minus'\n",
    "                    return winner\n",
    "            # check for horizontal winners\n",
    "            if col <= (ncol-4):\n",
    "                if np.sum(board[row,col:(col+4)])==4:\n",
    "                    winner = 'h-plus'\n",
    "                    return winner\n",
    "                elif np.sum(board[row,col:(col+4)])==-4:\n",
    "                    winner = 'h-minus'\n",
    "                    return winner\n",
    "            # check for top left to bottom right diagonal winners\n",
    "            if (row <= (nrow-4)) and (col <= (ncol-4)):\n",
    "                if np.sum(board[range(row,row+4),range(col,col+4)])==4:\n",
    "                    winner = 'd-plus'\n",
    "                    return winner\n",
    "                elif np.sum(board[range(row,row+4),range(col,col+4)])==-4:\n",
    "                    winner = 'd-minus'\n",
    "                    return winner\n",
    "            # check for top right to bottom left diagonal winners\n",
    "            if (row <= (nrow-4)) and (col >= 3):\n",
    "                if np.sum(board[range(row,row+4),range(col,col-4,-1)])==4:\n",
    "                    winner = 'd-plus'\n",
    "                    return winner\n",
    "                elif np.sum(board[range(row,row+4),range(col,col-4,-1)])==-4:\n",
    "                    winner = 'd-minus'\n",
    "                    return winner\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the Popout game board visually \n",
    "\n",
    "def display_board(board):\n",
    "    # this function displays the board as ascii using X for +1 and O for -1\n",
    "    clear_output()\n",
    "    horizontal_line = '-'*(7*5+8)\n",
    "    blank_line = '|'+' '*5\n",
    "    blank_line *= 7\n",
    "    blank_line += '|'\n",
    "    print(horizontal_line)\n",
    "    for row in range(6):\n",
    "        print(blank_line)\n",
    "        this_line = '|'\n",
    "        for col in range(7):\n",
    "            if board[row,col] == 0:\n",
    "                this_line += ' '*5 + '|'\n",
    "            elif board[row,col] == 1:\n",
    "                this_line += '  X  |'\n",
    "            else:\n",
    "                this_line += '  O  |'\n",
    "        print(this_line)\n",
    "        print(blank_line)\n",
    "        print(horizontal_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that checks if a move for a player is valid, given the board\n",
    "\n",
    "def valid_move(board, player):\n",
    "    valid_moves = []\n",
    "\n",
    "    # check where you can drop a checker\n",
    "    col_empty = np.where(board[0,:] == 0)[0]\n",
    "    for col in col_empty:\n",
    "        valid_moves.append(col)\n",
    "\n",
    "    # check where you can remove a checker\n",
    "    last_row = board[-1,:]\n",
    "    if player == 'plus':  # can only drop if their checker in last row\n",
    "        drop = np.where(last_row == 1)[0]\n",
    "        drop += 7\n",
    "    else :  # same case for minus player \n",
    "        drop = np.where(last_row == -1)[0]\n",
    "        drop += 7\n",
    "\n",
    "    for col in drop:\n",
    "        valid_moves.append(col)\n",
    "\n",
    "    return valid_moves    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to see if auto win is possible -- will also be used in reverse to see if a block is possible\n",
    "\n",
    "def auto_win(board, valid_moves, player) : \n",
    "    # play all valid moves and return winning move if possible\n",
    "    auto_wins = []\n",
    "    for move in valid_moves :\n",
    "        board_temp = update_board(board, player, move)\n",
    "        winner = check_for_win(board_temp)\n",
    "        if player in winner :  # if there is an auto win move, add it and return it\n",
    "            auto_wins.append(move)\n",
    "    return auto_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return discount rewards\n",
    "\n",
    "def discount_rewards(rewards, delta=0.90):  # use of delta = 0.9\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        running_add = running_add * delta + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1184c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the model architecture\n",
    "\n",
    "def create_model(height, width, channels):\n",
    "\n",
    "    imp = Input(shape=(height,width, channels))\n",
    "    mid = Conv2D(32,(2,2),strides=1,activation='relu')(imp)\n",
    "    mid = Conv2D(64,(2,2),strides=1,activation='relu')(mid)\n",
    "    mid = Conv2D(128,(2,2),strides=1,activation='relu')(mid)\n",
    "    mid = Flatten()(mid)\n",
    "    mid = Dense(256,activation='relu')(mid)\n",
    "    out = Dense(14,activation='softmax')(mid)  # softmax final layer to output probabilities for the actions\n",
    "    model = Model(imp,out) \n",
    "    \n",
    "    # compile model wtih sparse categorical crossentropy loss function\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer= tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to play a single game\n",
    "\n",
    "def play_game(epsilon):\n",
    "    winner = 'nobody'\n",
    "    board = np.zeros((6,7))\n",
    "    player = 'plus'\n",
    "\n",
    "    # toss coin to see who goes first\n",
    "    if np.random.rand() < 0.5:\n",
    "        player = 'plus'\n",
    "    else:\n",
    "        player = 'minus'\n",
    "\n",
    "    # initialize game data to store \n",
    "    game_data = {'boards_plus':[],'actions_plus':[], 'rewards_plus':[], 'discount_rewards_plus':[], 'boards_minus':[], 'actions_minus':[], 'rewards_minus':[], 'discount_rewards_minus':[], 'winner': 'nobody', 'win_type': 'nobody'}\n",
    "\n",
    "    # loop until there is a winner\n",
    "    while winner == 'nobody':\n",
    "\n",
    "        # get valid moves for player at that board\n",
    "        valid_moves = valid_move(board, player)\n",
    "        board_feed = board.reshape(1,6,7,1)\n",
    "\n",
    "        # get plus player's action\n",
    "        if player == 'plus':\n",
    "\n",
    "            # epsilon % of the time play random (legal) move\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(valid_moves)\n",
    "            \n",
    "            # 1-epsilon % of the time play the best move\n",
    "            else:\n",
    "\n",
    "                # get probabilities for each action from the plus model\n",
    "                mod_probs_plus = player_plus(board_feed, training=False).numpy()[0]\n",
    "                non_valid_moves = np.setdiff1d(np.arange(14), valid_moves)\n",
    "                mod_probs_plus[non_valid_moves] = 0  # set non valid moves to 0\n",
    "\n",
    "                if np.sum(mod_probs_plus) == 0:\n",
    "                    action = np.random.choice(valid_moves)\n",
    "                else:  \n",
    "                    # renormalize so probabilities sum to 1\n",
    "                    mod_probs_plus = mod_probs_plus/np.sum(mod_probs_plus)\n",
    "                    action = np.random.choice(np.arange(14), p=mod_probs_plus)\n",
    "            \n",
    "            # play auto win and block a fraction of the time so it can learn these moves\n",
    "            if np.random.rand() < epsilon/50:\n",
    "\n",
    "                # check for auto winning moves\n",
    "                winning_move = auto_win(board, valid_moves, player)\n",
    "                blocks = auto_win(board, valid_move(board, 'minus'), 'minus')\n",
    "\n",
    "                if len(winning_move) > 0:  \n",
    "                    action = np.random.choice(winning_move)   # play winning move if possible\n",
    "\n",
    "                elif len(blocks) > 0:\n",
    "                    action = np.random.choice(blocks)         # block if possible\n",
    "\n",
    "            # add that board to the game data\n",
    "            game_data['boards_plus'].append(board)\n",
    "\n",
    "        # get minus move\n",
    "        elif player == 'minus':\n",
    "            \n",
    "            # epsilon % of the time play random (legal) move\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(valid_moves)\n",
    "            \n",
    "            # 1-epsilon % of the time play the best move\n",
    "            else:\n",
    "                \n",
    "                # get probabilities for each action from the minus model\n",
    "                mod_probs_minus = player_minus(board_feed, training=False).numpy()[0]\n",
    "                non_valid_moves = np.setdiff1d(np.arange(14), valid_moves)\n",
    "                mod_probs_minus[non_valid_moves] = 0  # set non valid moves to 0\n",
    "                \n",
    "                if np.sum(mod_probs_minus) == 0:\n",
    "                    action = np.random.choice(valid_moves)\n",
    "                else:\n",
    "                    # renormalize so probabilities sum to 1\n",
    "                    mod_probs_minus = mod_probs_minus/np.sum(mod_probs_minus)\n",
    "                    action = np.random.choice(np.arange(14), p=mod_probs_minus)\n",
    "            \n",
    "            # play auto win and block a fraction of the time so it can learn these moves\n",
    "            if np.random.rand() < epsilon/50:\n",
    "                \n",
    "                # check for auto winning moves\n",
    "                winning_move = auto_win(board, valid_moves, player)\n",
    "                blocks = auto_win(board, valid_move(board, 'plus'), 'plus')\n",
    "\n",
    "                if len(winning_move) > 0:  \n",
    "                    action = np.random.choice(winning_move)   # play winning move if possible\n",
    "\n",
    "                elif len(blocks) > 0:\n",
    "                    action = np.random.choice(blocks)      # block if possible\n",
    "\n",
    "            # add that board to the game data\n",
    "            game_data['boards_minus'].append(board)\n",
    "        \n",
    "        # update board and check for win\n",
    "        board = update_board(board, player, action)\n",
    "        winner = check_for_win(board)\n",
    "        \n",
    "        # add winner and win type to game data\n",
    "        if winner != 'nobody' and 'plus' in winner:\n",
    "            game_data['winner'] = 'plus'\n",
    "            game_data['win_type'] = winner\n",
    "        elif winner != 'nobody' and 'minus' in winner:\n",
    "            game_data['winner'] = 'minus'\n",
    "            game_data['win_type'] = winner\n",
    "        \n",
    "        # switch player\n",
    "        if player == 'plus':\n",
    "            # store action taken and reward of 0 if no winner yet\n",
    "            game_data['actions_plus'].append(action)\n",
    "            game_data['rewards_plus'].append(0)\n",
    "\n",
    "            # give reward to winner of 1, and loser a -5\n",
    "            if 'plus' in winner :\n",
    "                game_data['rewards_plus'][-1] = 1\n",
    "                game_data['rewards_minus'][-1] = -5\n",
    "            elif 'minus' in winner:\n",
    "                game_data['rewards_plus'][-1] = -5\n",
    "                game_data['rewards_minus'][-1] = 1\n",
    "\n",
    "            player = 'minus'\n",
    "\n",
    "        else:\n",
    "            # store action taken and reward of 0 if no winner yet\n",
    "            game_data['actions_minus'].append(action)\n",
    "            game_data['rewards_minus'].append(0)\n",
    "\n",
    "            # give reward to winner of 1, and loser a -5\n",
    "            if 'plus' in winner :\n",
    "                game_data['rewards_plus'][-1] = 1\n",
    "                game_data['rewards_minus'][-1] = -5\n",
    "            elif 'minus' in winner:\n",
    "                game_data['rewards_plus'][-1] = -5\n",
    "                game_data['rewards_minus'][-1] = 1\n",
    "\n",
    "\n",
    "            player = 'plus'\n",
    "\n",
    "    # add discounted rewards to game data\n",
    "    game_data['discount_rewards_plus'] = discount_rewards(game_data['rewards_plus'])\n",
    "    game_data['discount_rewards_minus'] = discount_rewards(game_data['rewards_minus'])\n",
    "\n",
    "    return game_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check how the plus model performs against a random (valid) player\n",
    "\n",
    "def check_plus_model_wins (model, trials) : \n",
    "\n",
    "    # track win count\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "\n",
    "    # loop through trials\n",
    "    for trial in range(trials):\n",
    "\n",
    "        winner = 'nobody'\n",
    "        board = np.zeros((6,7))\n",
    "        player = 'plus'\n",
    "        moves = 0\n",
    "\n",
    "        # play game\n",
    "        while winner == 'nobody':\n",
    "            \n",
    "            # get valid moves\n",
    "            valid_moves = valid_move(board, player)\n",
    "            board_feed = board.reshape(1,6,7,1)\n",
    "\n",
    "            # get move from plus model\n",
    "            if player == 'plus':\n",
    "                \n",
    "                # get probabilities for each action from the plus model\n",
    "                mod_probs_plus = model(board_feed, training=False).numpy()[0]\n",
    "                non_valid_moves = np.setdiff1d(np.arange(14), valid_moves)\n",
    "                mod_probs_plus[non_valid_moves] = 0  # set non valid moves to 0\n",
    "\n",
    "                if np.sum(mod_probs_plus) == 0:\n",
    "                    action = np.random.choice(valid_moves)\n",
    "                else:\n",
    "\n",
    "                    # renormalize probabilities so they sum to 1\n",
    "                    mod_probs_plus = mod_probs_plus/np.sum(mod_probs_plus)\n",
    "                    action = np.random.choice(np.arange(14), p=mod_probs_plus)\n",
    "\n",
    "            # get random move from minus player (random player here)\n",
    "            elif player == 'minus':\n",
    "                # check if there is a valid move\n",
    "                valid_moves = valid_move(board, player)\n",
    "                action = np.random.choice(valid_moves)\n",
    "\n",
    "            # update board and check for win\n",
    "            board = update_board(board,player,action)\n",
    "            winner = check_for_win(board)\n",
    "            moves += 1\n",
    "\n",
    "            # switch player\n",
    "            if player == 'plus':\n",
    "                player = 'minus'\n",
    "            else:\n",
    "                player = 'plus'\n",
    "        \n",
    "        # update win count\n",
    "        if 'plus' in winner:\n",
    "            plus += 1\n",
    "        elif 'minus' in winner:\n",
    "            minus += 1\n",
    "\n",
    "    # calculate win rates\n",
    "    plus_wins = plus/trials\n",
    "    rand_minus_wins = minus/trials\n",
    "\n",
    "    return plus_wins, rand_minus_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check how the minus model performs against a random (valid) player\n",
    "\n",
    "def check_minus_model_wins(model, trials) : \n",
    "\n",
    "    # track win count\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "\n",
    "    # loop through trials\n",
    "    for trial in range(trials):\n",
    "\n",
    "        winner = 'nobody'\n",
    "        board = np.zeros((6,7))\n",
    "        player = 'minus'\n",
    "        moves = 0\n",
    "\n",
    "        # play game\n",
    "        while winner == 'nobody':\n",
    "            \n",
    "            # get valid moves\n",
    "            valid_moves = valid_move(board, player)\n",
    "            board_feed = board.reshape(1,6,7,1)\n",
    "\n",
    "            # this is the random player\n",
    "            if player == 'plus':\n",
    "\n",
    "                # check if there is a valid move\n",
    "                valid_moves = valid_move(board, player)\n",
    "                action = np.random.choice(valid_moves)\n",
    "\n",
    "            # get move from minus model\n",
    "            elif player == 'minus':\n",
    "                \n",
    "                # get probabilities for each action from the minus model\n",
    "                mod_probs_minus = player_minus(board_feed, training=False).numpy()[0]\n",
    "                non_valid_moves = np.setdiff1d(np.arange(14), valid_moves)\n",
    "                mod_probs_minus[non_valid_moves] = 0  # set non valid moves to 0\n",
    "\n",
    "                if np.sum(mod_probs_minus) == 0:\n",
    "                    action = np.random.choice(valid_moves)\n",
    "                else:\n",
    "                    # renormalize probabilities so they sum to 1\n",
    "                    mod_probs_minus = mod_probs_minus/np.sum(mod_probs_minus)\n",
    "                    action = np.random.choice(np.arange(14), p=mod_probs_minus)\n",
    "\n",
    "            # update board and check for win\n",
    "            board = update_board(board,player,action)\n",
    "            winner = check_for_win(board)\n",
    "            moves += 1\n",
    "\n",
    "            # switch player\n",
    "            if player == 'plus':\n",
    "                player = 'minus'\n",
    "            else:\n",
    "                player = 'plus'\n",
    "\n",
    "        # update win count\n",
    "        if 'plus' in winner:\n",
    "            plus += 1\n",
    "        elif 'minus' in winner:\n",
    "            minus += 1\n",
    "\n",
    "    # calculate win rates\n",
    "    rand_plus_wins = plus/trials\n",
    "    minus_wins = minus/trials\n",
    "\n",
    "    return rand_plus_wins, minus_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried a different network architecture than was in our DQN model. This model had the same number of convolutional layers and dense layers, however, here, we used smaller filter dimensions for the first convolutional layer (2x2 vs. 4v4). We found this architecture to work better for the PG models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a656531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 6, 7, 1)]         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 5, 6, 32)          160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 4, 5, 64)          8256      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 4, 128)         32896     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               393472    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 14)                3598      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 438,382\n",
      "Trainable params: 438,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize plus and minus models \n",
    "player_plus = create_model(6,7,1)\n",
    "player_minus = create_model(6,7,1)\n",
    "\n",
    "player_plus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speeds up training \n",
    "player_plus.call = tf.function(player_plus.call, experimental_relax_shapes=True)\n",
    "player_minus.call = tf.function(player_minus.call, experimental_relax_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Buffer and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our memory buffer, we built a buffer for the plus player and the minus player, and stored the current boards, actions, and discounted rewards for each player. We kept a small buffer of 1000 boards for each player so that the model was not learning on very old moves and boards. With each step of SGD, we sample 50 boards and compute the loss (sparse categorical cross entropy) betweem the actions that were actually taken and the highest probability that the model output. We train until we reach the limit of total boards of 1,000,000 boards. This resulted in roughly 150,000 games. We also added linear annealing on top of the PG technique so that the PG models wouldn't learn to quickly against a bad model early on. This added another layer of randomness so the models wouldn't learn too quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the buffer\n",
    "batch_size = 50\n",
    "buffn = 1000\n",
    "warmupboards = 500\n",
    "max_boards = 1000000 + warmupboards\n",
    "tot_boards = 0\n",
    "len_buffer_plus = 0\n",
    "len_buffer_minus = 0\n",
    "buffer_plus = {'boards':[],'actions':[],'rewards':[]}\n",
    "buffer_minus = {'boards':[],'actions':[],'rewards':[]}\n",
    "\n",
    "# sparse categorical crossentropy loss object\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# linear annealing parameters \n",
    "anneal1 = 600000\n",
    "anneal2 = 300000\n",
    "\n",
    "ep0 = 1.0\n",
    "ep1 = 0.5\n",
    "ep2 = 0.05\n",
    "epsilon = ep0\n",
    "dep1 = (ep0-ep1)/anneal1\n",
    "dep2 = (ep1-ep2)/anneal2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game:  2000 \ttime:  462.4003789424896 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  17 \tminus boards:  17 \tepsilon:  0.9773691666666656\n",
      "game:  4000 \ttime:  483.31683826446533 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-minus \tplus boards:  6 \tminus boards:  6 \tepsilon:  0.9547124999999972\n",
      "game:  6000 \ttime:  482.01316571235657 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.9325899999999944\n",
      "game:  8000 \ttime:  469.3741900920868 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-plus \tplus boards:  13 \tminus boards:  14 \tepsilon:  0.9111391666666544\n",
      "game:  10000 \ttime:  457.8006341457367 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 d-plus \tplus boards:  18 \tminus boards:  17 \tepsilon:  0.88956666666665\n",
      "game:  12000 \ttime:  453.2348470687866 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-plus \tplus boards:  14 \tminus boards:  13 \tepsilon:  0.8688008333333108\n",
      "game:  14000 \ttime:  460.8054928779602 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 d-plus \tplus boards:  17 \tminus boards:  17 \tepsilon:  0.8483349999999727\n",
      "game:  16000 \ttime:  449.85578203201294 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  16 \tminus boards:  15 \tepsilon:  0.8281216666666311\n",
      "game:  18000 \ttime:  444.153874874115 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-minus \tplus boards:  20 \tminus boards:  20 \tepsilon:  0.8084916666666226\n",
      "game:  20000 \ttime:  456.75475001335144 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-plus \tplus boards:  12 \tminus boards:  12 \tepsilon:  0.7887774999999488\n",
      "game:  22000 \ttime:  449.05424070358276 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  9 \tminus boards:  9 \tepsilon:  0.7695541666666063\n",
      "game:  24000 \ttime:  455.4329881668091 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-minus \tplus boards:  15 \tminus boards:  15 \tepsilon:  0.7503891666665942\n",
      "game:  26000 \ttime:  453.6288011074066 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  25 \tminus boards:  25 \tepsilon:  0.732218333333246\n",
      "game:  28000 \ttime:  460.0545222759247 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  12 \tminus boards:  11 \tepsilon:  0.7136033333332358\n",
      "game:  30000 \ttime:  455.82303404808044 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  5 \tminus boards:  5 \tepsilon:  0.6949341666665582\n",
      "game:  32000 \ttime:  453.49579215049744 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-plus \tplus boards:  10 \tminus boards:  9 \tepsilon:  0.6767316666665462\n",
      "game:  34000 \ttime:  436.6887950897217 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-plus \tplus boards:  15 \tminus boards:  15 \tepsilon:  0.6610349999998549\n",
      "game:  36000 \ttime:  417.4534947872162 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  8 \tminus boards:  9 \tepsilon:  0.646109166666488\n",
      "game:  38000 \ttime:  436.766695022583 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  10 \tminus boards:  10 \tepsilon:  0.6312566666664555\n",
      "game:  40000 \ttime:  420.72313499450684 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  6 \tminus boards:  6 \tepsilon:  0.6175749999997494\n",
      "game:  42000 \ttime:  416.61018085479736 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-plus \tplus boards:  7 \tminus boards:  6 \tepsilon:  0.6043383333330399\n",
      "game:  44000 \ttime:  379.7769989967346 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  9 \tminus boards:  8 \tepsilon:  0.5914641666663263\n",
      "game:  46000 \ttime:  384.2235321998596 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-plus \tplus boards:  20 \tminus boards:  20 \tepsilon:  0.5789299999996098\n",
      "game:  48000 \ttime:  380.15424489974976 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.5667533333328943\n",
      "game:  50000 \ttime:  383.10684299468994 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 h-plus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.5546341666661795\n",
      "game:  52000 \ttime:  384.6344699859619 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.5428708333327953\n",
      "game:  54000 \ttime:  381.153107881546 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  5 \tminus boards:  6 \tepsilon:  0.5312899999994113\n",
      "game:  56000 \ttime:  375.32396697998047 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  6 \tminus boards:  6 \tepsilon:  0.520009999999359\n",
      "game:  58000 \ttime:  378.881236076355 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  5 \tminus boards:  6 \tepsilon:  0.5088041666659743\n",
      "game:  60000 \ttime:  370.1570188999176 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.49649666666593084\n",
      "game:  62000 \ttime:  372.46647810935974 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.4772096666659297\n",
      "game:  64000 \ttime:  376.08746099472046 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  11 \tminus boards:  11 \tepsilon:  0.45859916666592815\n",
      "game:  66000 \ttime:  370.25186109542847 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  8 \tminus boards:  7 \tepsilon:  0.44069366666592585\n",
      "game:  68000 \ttime:  370.61661100387573 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  5 \tminus boards:  5 \tepsilon:  0.4232591666659234\n",
      "game:  70000 \ttime:  366.1295771598816 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  6 \tminus boards:  6 \tepsilon:  0.40626116666592205\n",
      "game:  72000 \ttime:  362.3292648792267 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.3897161666659208\n",
      "game:  74000 \ttime:  368.78355717658997 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.37344866666591875\n",
      "game:  76000 \ttime:  365.5232470035553 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  5 \tminus boards:  6 \tepsilon:  0.35797616666591425\n",
      "game:  78000 \ttime:  369.137690782547 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.3426341666659114\n",
      "game:  80000 \ttime:  377.55829191207886 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.3274466666659078\n",
      "game:  82000 \ttime:  382.0444800853729 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  7 \tminus boards:  8 \tepsilon:  0.31265066666590285\n",
      "game:  84000 \ttime:  383.2342391014099 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.29817866666589843\n",
      "game:  86000 \ttime:  387.9112651348114 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.283903166665893\n",
      "game:  88000 \ttime:  380.66314005851746 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.2695091666658883\n",
      "game:  90000 \ttime:  376.79618191719055 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  5 \tminus boards:  5 \tepsilon:  0.25586216666588074\n",
      "game:  92000 \ttime:  367.74765706062317 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.24224516666587695\n",
      "game:  94000 \ttime:  375.77441787719727 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.2289281666658761\n",
      "game:  96000 \ttime:  393.50160789489746 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.2159906666658755\n",
      "game:  98000 \ttime:  376.7257342338562 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.20326916666587488\n",
      "game:  100000 \ttime:  375.1664779186249 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.19072616666587405\n",
      "game:  102000 \ttime:  362.2241966724396 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  6 \tminus boards:  6 \tepsilon:  0.17817566666587312\n",
      "game:  104000 \ttime:  356.7318859100342 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.16592666666587308\n",
      "game:  106000 \ttime:  379.0647518634796 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.15382766666587225\n",
      "game:  108000 \ttime:  358.74734473228455 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.14180516666587176\n",
      "game:  110000 \ttime:  359.81500601768494 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.1300991666658719\n",
      "game:  112000 \ttime:  365.64999866485596 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.11647616666587125\n",
      "game:  114000 \ttime:  371.056272983551 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.10278866666586887\n",
      "game:  116000 \ttime:  363.489052772522 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.08930516666586855\n",
      "game:  118000 \ttime:  351.3313217163086 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  5 \tminus boards:  4 \tepsilon:  0.07690766666586482\n",
      "game:  120000 \ttime:  333.73185300827026 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.0658001666658555\n",
      "game:  122000 \ttime:  335.22838521003723 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.05481866666585468\n",
      "game:  124000 \ttime:  351.6066999435425 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  126000 \ttime:  335.51398181915283 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.049997666665856\n",
      "game:  128000 \ttime:  353.6962077617645 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.049997666665856\n",
      "game:  130000 \ttime:  349.50202322006226 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  132000 \ttime:  355.59142088890076 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  134000 \ttime:  346.56171703338623 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.049997666665856\n",
      "game:  136000 \ttime:  340.06795287132263 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  138000 \ttime:  352.44421315193176 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  140000 \ttime:  352.672238111496 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  142000 \ttime:  347.35115003585815 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  144000 \ttime:  354.6383640766144 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  146000 \ttime:  355.19198775291443 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.049997666665856\n",
      "game:  148000 \ttime:  357.2276999950409 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-minus \tplus boards:  3 \tminus boards:  4 \tepsilon:  0.049997666665856\n",
      "game:  150000 \ttime:  355.89674639701843 \tlen_buffer_plus:  1000 \tlen_buffer_minus:  1000 v-plus \tplus boards:  4 \tminus boards:  3 \tepsilon:  0.049997666665856\n"
     ]
    }
   ],
   "source": [
    "# begin training loop\n",
    "start = time.time()\n",
    "\n",
    "# set up dataframe to store results \n",
    "save_output = pd.DataFrame(columns=['game','runtime','epsilon','plus_wins','rand_minus_wins','rand_plus_wins','minus_wins'])\n",
    "\n",
    "# lists to store winner info \n",
    "winner = []\n",
    "win_type = []\n",
    "plus = [] \n",
    "minus = []\n",
    "games = 0\n",
    "\n",
    "# loop until upper limit of total boards is reached\n",
    "while tot_boards < max_boards: \n",
    "\n",
    "    # play a game and increment game counter\n",
    "    game_data = play_game(epsilon)\n",
    "    games += 1\n",
    "\n",
    "    # add winner and win type to lists \n",
    "    winner.append(game_data['winner'])\n",
    "    win_type.append(game_data['win_type'])\n",
    "\n",
    "    # add to lists storing win counts per player (1: win, 0: loss)\n",
    "    if game_data['winner'] == 'plus':\n",
    "        plus.append(1)\n",
    "        minus.append(0)\n",
    "    elif game_data['winner'] == 'minus':\n",
    "        plus.append(0)\n",
    "        minus.append(1)\n",
    "\n",
    "    # extend the plus and minus buffers with the frames from the game played, actions, and discounted rewards\n",
    "    buffer_plus['boards'].extend(game_data['boards_plus'])\n",
    "    buffer_plus['actions'].extend(game_data['actions_plus'])\n",
    "    buffer_plus['rewards'].extend(game_data['discount_rewards_plus'])\n",
    "\n",
    "    buffer_minus['boards'].extend(game_data['boards_minus'])\n",
    "    buffer_minus['actions'].extend(game_data['actions_minus'])\n",
    "    buffer_minus['rewards'].extend(game_data['discount_rewards_minus'])\n",
    "\n",
    "    # add to tot_boards and each buffer length \n",
    "    tot_boards += len(game_data['boards_plus'])\n",
    "    len_buffer_plus += len(game_data['boards_plus'])\n",
    "    len_buffer_minus += len(game_data['boards_minus'])\n",
    "\n",
    "    # remove excess buffer for both when exceed 1000 boards\n",
    "    if len_buffer_plus > buffn:\n",
    "        excess = len_buffer_plus - buffn\n",
    "        buffer_plus['boards'] = buffer_plus['boards'][excess:].copy()\n",
    "        buffer_plus['actions'] = buffer_plus['actions'][excess:].copy()\n",
    "        buffer_plus['rewards'] = buffer_plus['rewards'][excess:].copy()\n",
    "        len_buffer_plus = len(buffer_plus['actions'])\n",
    "    if len_buffer_minus > buffn:\n",
    "        excess = len_buffer_minus - buffn\n",
    "        buffer_minus['boards'] = buffer_minus['boards'][excess:].copy()\n",
    "        buffer_minus['actions'] = buffer_minus['actions'][excess:].copy()\n",
    "        buffer_minus['rewards'] = buffer_minus['rewards'][excess:].copy()\n",
    "        len_buffer_minus = len(buffer_minus['actions'])\n",
    "    \n",
    "    # once both buffers longer than warm up boards, begin two stage annealing \n",
    "    if len_buffer_plus > warmupboards and len_buffer_minus > warmupboards:\n",
    "\n",
    "        # first stage of annealing\n",
    "        if epsilon >= ep1:\n",
    "            epsilon -= dep1 * len(game_data['boards_plus'])\n",
    "        # second stage of annealing\n",
    "        elif epsilon < ep1 and epsilon > ep2:\n",
    "            epsilon -= dep2 * len(game_data['boards_plus'])\n",
    "\n",
    "    # start training plus model once buffer is longer than warm up boards\n",
    "    if len_buffer_plus > warmupboards :\n",
    "\n",
    "        # oversample boards with positive rewards\n",
    "        prob_plus = np.ones(len_buffer_plus)\n",
    "        prob_plus[np.array(buffer_plus['rewards']) > 0] = 3\n",
    "        prob_plus = prob_plus / np.sum(prob_plus)\n",
    "        # select random sample from buffer\n",
    "        which_choose_plus = np.random.choice(len_buffer_plus, batch_size, replace=False, p=prob_plus)\n",
    "\n",
    "        # use random indices to extract boards, actions, and rewards from buffer \n",
    "        boards = np.array(buffer_plus['boards'])[which_choose_plus]\n",
    "        actions = np.array(buffer_plus['actions'])[which_choose_plus]\n",
    "        rewards = np.array(buffer_plus['rewards'])[which_choose_plus]\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        # train model\n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward pass\n",
    "            probs = player_plus(boards)\n",
    "            loss_value = tf.losses.sparse_categorical_crossentropy(actions, probs) * tf.constant(rewards, dtype = tf.float32)\n",
    "            # backward pass\n",
    "            gradients = tape.gradient(loss_value, player_plus.trainable_variables)\n",
    "            # update weights\n",
    "            optimizer.apply_gradients(zip(gradients, player_plus.trainable_variables))\n",
    "\n",
    "    # start training minus model once buffer is longer than warm up boards    \n",
    "    if len_buffer_minus > warmupboards :\n",
    "\n",
    "        # oversample boards with positive rewards\n",
    "        prob_minus = np.ones(len_buffer_minus)\n",
    "        prob_minus[np.array(buffer_minus['rewards']) > 0] = 3\n",
    "        prob_minus = prob_minus / np.sum(prob_minus)\n",
    "        # select random sample from buffer\n",
    "        which_choose_minus = np.random.choice(len_buffer_minus, batch_size, replace=False, p=prob_minus)\n",
    "\n",
    "        # get random sample from buffer\n",
    "        boards = np.array(buffer_minus['boards'])[which_choose_minus]\n",
    "        actions = np.array(buffer_minus['actions'])[which_choose_minus]\n",
    "        rewards = np.array(buffer_minus['rewards'])[which_choose_minus]\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        # train model\n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward pass\n",
    "            probs = player_minus(boards)\n",
    "            loss_value = tf.losses.sparse_categorical_crossentropy(actions, probs) * tf.constant(rewards, dtype = tf.float32)\n",
    "            # backward pass\n",
    "            gradients = tape.gradient(loss_value, player_minus.trainable_variables)\n",
    "            # update weights\n",
    "            optimizer.apply_gradients(zip(gradients, player_minus.trainable_variables))\n",
    "\n",
    "    # save models and output every 2000 games to track progress\n",
    "    if games % 2000 == 0:\n",
    "        player_plus.save('models/player_plus_{0}.h5'.format(games))\n",
    "        player_minus.save('models/player_minus_{0}.h5'.format(games))\n",
    "        end = time.time()\n",
    "        # add to df\n",
    "        plus_wins, rand_minus_wins = check_plus_model_wins(player_plus, 1000)\n",
    "        rand_plus_wins, minus_wins = check_minus_model_wins(player_minus, 1000)\n",
    "        save_output.loc[len(save_output)] = [games, end-start, epsilon, plus_wins, rand_minus_wins, rand_plus_wins, minus_wins]\n",
    "        save_output.to_csv('save_output.csv', index=False)\n",
    "        print('game: ', games, '\\ttime: ', end-start, '\\tlen_buffer_plus: ', len_buffer_plus, '\\tlen_buffer_minus: ', len_buffer_minus, game_data['win_type'], '\\tplus boards: ', len(game_data['boards_plus']), '\\tminus boards: ', len(game_data['boards_minus']), '\\tepsilon: ', epsilon)\n",
    "        start = time.time()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plus     0.518664\n",
       "minus    0.481336\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see split between plus and minus wins\n",
    "winner = pd.Series(winner)\n",
    "winner.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v-plus     0.411214\n",
       "v-minus    0.403921\n",
       "h-plus     0.089175\n",
       "h-minus    0.061154\n",
       "d-plus     0.018275\n",
       "d-minus    0.016261\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see distribution of win types \n",
    "win_type = pd.Series(win_type)\n",
    "win_type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Test Model Against Random Opponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model against a random opponent, use the cell below and import the necessary packages at the top of this notebook. You will also need to run the following functions from the Basic Functions section at the top of this notebook:  \n",
    "\n",
    "- **update_board()**: Updates game board after moves\n",
    "- **check_for_win()**: Checks board for a winner\n",
    "- **valid_move()**: Restricts both players from making invalid moves\n",
    "- **check_plus_model_wins()**: Checks our model's win rate against a random opponent\n",
    "\n",
    "The valid move function restricts both our plus model and the random opponent from making invalid moves in its construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus Win Rate: 0.991\n",
      "Random Win Rate: 0.009\n"
     ]
    }
   ],
   "source": [
    "# read in h5 model submitted to Canvas\n",
    "player_plus = tf.keras.models.load_model('pg_model.h5') \n",
    "player_plus.call = tf.function(player_plus.call, experimental_relax_shapes=True)\n",
    "\n",
    "# calculates win rate of plus model against random for 1000 games\n",
    "plus_wins, rand_minus_wins = check_plus_model_wins(player_plus, 1000)\n",
    "print('Plus Win Rate:', plus_wins)\n",
    "print('Random Win Rate:', rand_minus_wins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fda165f50e76d432f6ed799118bc9dab23fa79194393d6371ca86a52c69e7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
